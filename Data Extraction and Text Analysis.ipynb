{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "def4c573",
   "metadata": {},
   "source": [
    "## Data Extraction and Text Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d421b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be5f3ffc",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './cik_list.xlsx'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m cik_list \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_excel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./cik_list.xlsx\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m max_row, max_col \u001b[38;5;241m=\u001b[39m cik_list\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(max_row)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\excel\\_base.py:478\u001b[0m, in \u001b[0;36mread_excel\u001b[1;34m(io, sheet_name, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, date_format, thousands, decimal, comment, skipfooter, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m    476\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(io, ExcelFile):\n\u001b[0;32m    477\u001b[0m     should_close \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 478\u001b[0m     io \u001b[38;5;241m=\u001b[39m \u001b[43mExcelFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    479\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m engine \u001b[38;5;129;01mand\u001b[39;00m engine \u001b[38;5;241m!=\u001b[39m io\u001b[38;5;241m.\u001b[39mengine:\n\u001b[0;32m    480\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    481\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEngine should not be specified when passing \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    482\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124man ExcelFile - ExcelFile already has the engine set\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    483\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\excel\\_base.py:1496\u001b[0m, in \u001b[0;36mExcelFile.__init__\u001b[1;34m(self, path_or_buffer, engine, storage_options)\u001b[0m\n\u001b[0;32m   1494\u001b[0m     ext \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxls\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1495\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1496\u001b[0m     ext \u001b[38;5;241m=\u001b[39m \u001b[43minspect_excel_format\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontent_or_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\n\u001b[0;32m   1498\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1499\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ext \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1501\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExcel file format cannot be determined, you must specify \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1502\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124man engine manually.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1503\u001b[0m         )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\excel\\_base.py:1371\u001b[0m, in \u001b[0;36minspect_excel_format\u001b[1;34m(content_or_path, storage_options)\u001b[0m\n\u001b[0;32m   1368\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(content_or_path, \u001b[38;5;28mbytes\u001b[39m):\n\u001b[0;32m   1369\u001b[0m     content_or_path \u001b[38;5;241m=\u001b[39m BytesIO(content_or_path)\n\u001b[1;32m-> 1371\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1372\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontent_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[0;32m   1373\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m handle:\n\u001b[0;32m   1374\u001b[0m     stream \u001b[38;5;241m=\u001b[39m handle\u001b[38;5;241m.\u001b[39mhandle\n\u001b[0;32m   1375\u001b[0m     stream\u001b[38;5;241m.\u001b[39mseek(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\common.py:868\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    859\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    860\u001b[0m             handle,\n\u001b[0;32m    861\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    864\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    865\u001b[0m         )\n\u001b[0;32m    866\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    867\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m--> 868\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    869\u001b[0m     handles\u001b[38;5;241m.\u001b[39mappend(handle)\n\u001b[0;32m    871\u001b[0m \u001b[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './cik_list.xlsx'"
     ]
    }
   ],
   "source": [
    "cik_list = pd.read_excel(\"./cik_list.xlsx\")\n",
    "max_row, max_col = cik_list.shape\n",
    "print(max_row)\n",
    "\n",
    "pd.set_option('display.max_colwidth',100) # to display full text in column\n",
    "cik_list.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8f9e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "cik_list.SECFNAME.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60fd4411",
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding the initial structure of the link in secfname\n",
    "link = 'https://www.sec.gov/Archives/'\n",
    "cik_list.SECFNAME = link+cik_list.SECFNAME\n",
    "cik_list.SECFNAME.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f5d509",
   "metadata": {},
   "source": [
    "# Scraping data from the .txt files\n",
    "For each report, we would need three sections\n",
    "\n",
    "“Management's Discussion and Analysis”,\n",
    "“Quantitative and Qualitative Disclosures about Market Risk”\n",
    "“Risk Factors”\n",
    "The sections have specific pattern:\n",
    "ITEM (section_number). section_name (start)\n",
    "\n",
    "section_content (body)\n",
    "\n",
    "ITEM (next_section_number) or SIGNATURES section (if section is the last one) (end)\n",
    "\n",
    "Special case\n",
    "If the form type starts with \"NT\" the theres is not data in the form so we dont need to go through them\n",
    "\n",
    "Text preprocessing\n",
    "Noise Removal\n",
    "Tokenization\n",
    "Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a182ff13",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nltk'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrequests\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m\u001b[38;5;241m,\u001b[39m \u001b[38;5;21;01mstring\u001b[39;00m\u001b[38;5;241m,\u001b[39m \u001b[38;5;21;01municodedata\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbs4\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BeautifulSoup\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m word_tokenize, sent_tokenize\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'nltk'"
     ]
    }
   ],
   "source": [
    "#varies imports\n",
    "\n",
    "import requests\n",
    "import re, string, unicodedata\n",
    "import nltk\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b49e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "#making the stopword set from basic english and the given list of stopwords\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "stopset = set(w.upper() for w in stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257f57bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding more stopwords from text file of stopwords\n",
    "import glob\n",
    "path = \"StopWords*.txt\"\n",
    "glob.glob(path)\n",
    "for filename in glob.glob(path):\n",
    "    with open(filename, 'r') as f:\n",
    "        text = f.read()\n",
    "        text = re.sub(r\"\\s+\\|\\s+[\\w]*\" , \"\", text)        \n",
    "        stopset.update(text.upper().split())\n",
    "        #print(len(stopset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b938d0eb",
   "metadata": {},
   "source": [
    "# In the following section a lot of useful fuctions are defined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3fc3f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# syllables count (will be used in complex word count)\n",
    "from nltk.corpus import cmudict\n",
    "nltk.download('cmudict')\n",
    "d = cmudict.dict()\n",
    "\n",
    "def syllables(word):\n",
    "    #referred from stackoverflow.com/questions/14541303/count-the-number-of-syllables-in-a-word\n",
    "    count = 0\n",
    "    vowels = 'aeiouy'\n",
    "    word = word.lower()\n",
    "    if word[0] in vowels:\n",
    "        count +=1\n",
    "    for index in range(1,len(word)):\n",
    "        if word[index] in vowels and word[index-1] not in vowels:\n",
    "            count +=1\n",
    "    if word.endswith('e'):\n",
    "        count -= 1\n",
    "    if word.endswith('le'):\n",
    "        count+=1\n",
    "    if count == 0:\n",
    "        count +=1\n",
    "    return count\n",
    "\n",
    "def nsyl(word):\n",
    "    try:\n",
    "        return max([len(list(y for y in x if y[-1].isdigit())) for x in d[word.lower()]])\n",
    "    except KeyError:\n",
    "        #if word not found in cmudict\n",
    "        return syllables(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02117a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# other usefull functions\n",
    "def strip_html(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    return soup.get_text()\n",
    "\n",
    "def remove_between_square_brackets(text):\n",
    "    return re.sub('\\[[^]]*\\]', '', text)\n",
    "\n",
    "def remove_digits(text):\n",
    "    return re.sub('[\\d%/$]', '', text)\n",
    "\n",
    "def denoise_text(text):\n",
    "    text = strip_html(text)\n",
    "    text = remove_between_square_brackets(text)\n",
    "    text = remove_digits(text)\n",
    "    return text\n",
    "\n",
    "def remove_non_ascii(words):\n",
    "    \"\"\"Remove non-ASCII characters from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def to_upper_case(words):\n",
    "    \"\"\"Convert all characters to lowercase from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = word.upper()\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def remove_punctuation(words):\n",
    "    \"\"\"Remove punctuation from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = re.sub(r'[^\\w\\s]', '', word)\n",
    "        if new_word != '':\n",
    "            new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "\n",
    "def remove_stopwords(words):\n",
    "    \"\"\"Remove stop words from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word not in stopset:\n",
    "            new_words.append(word)\n",
    "    return new_words\n",
    "\n",
    "# def stem_words(words):\n",
    "#     \"\"\"Stem words in list of tokenized words\"\"\"\n",
    "#     stemmer = LancasterStemmer()\n",
    "#     stems = []\n",
    "#     for word in words:\n",
    "#         stem = stemmer.stem(word)\n",
    "#         stems.append(stem)\n",
    "#     return stems\n",
    "\n",
    "# def lemmatize_verbs(words):\n",
    "#     \"\"\"Lemmatize verbs in list of tokenized words\"\"\"\n",
    "#     lemmatizer = WordNetLemmatizer()\n",
    "#     lemmas = []\n",
    "#     for word in words:\n",
    "#         lemma = lemmatizer.lemmatize(word, pos='v')\n",
    "#         lemmas.append(lemma)\n",
    "#     return lemmas\n",
    "\n",
    "def normalize(words):\n",
    "    words = remove_non_ascii(words)\n",
    "    words = to_upper_case(words)\n",
    "    words = remove_punctuation(words)\n",
    "#     words = replace_numbers(words)\n",
    "    words = remove_stopwords(words)\n",
    "    return words\n",
    "\n",
    "# def stem_and_lemmatize(words):\n",
    "#     stems = stem_words(words)\n",
    "#     lemmas = lemmatize_verbs(words)\n",
    "#     return stems, lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694fa196",
   "metadata": {},
   "outputs": [],
   "source": [
    "# section names\n",
    "MDA = \"Management's Discussion and Analysis\"\n",
    "QQDMR = \"Quantitative and Qualitative Disclosures about Market Risk\"\n",
    "RF = \"Risk Factors\"\n",
    "section_name = ['MDA','QQDMR',\"RF\"]\n",
    "section = [MDA.upper(),QQDMR.upper(),RF.upper()]\n",
    "variables = ['positive_score','negative_score','polarity_score','average_sentence_length', 'percentage_of_complex_words',\\\n",
    "                   'fog_index','complex_word_count','word_count','uncertainty_score','constraining_score', 'positive_word_proportion',\\\n",
    "                   'negative_word_proportion', 'uncertainty_word_proportion', 'constraining_word_proportion' ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6120563",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "constraining_words_whole_report = pd.Series(name='constraining_words_whole_report')\n",
    "\n",
    "df_col = [sec.lower() + '_' + var for sec,var in itertools.product(section_name,variables) ]\n",
    "df = pd.DataFrame(columns=df_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9001b156",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc39bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#usefull dictionaries\n",
    "master_dict = pd.read_csv('./LoughranMcDonald_MasterDictionary_2016.csv', index_col= 0)\n",
    "constraining_dict = set(pd.read_excel('./constraining_dictionary.xlsx',index_col = 0).index)\n",
    "uncertainty_dict = set(pd.read_excel('./uncertainty_dictionary.xlsx', index_col = 0).index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33547f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cik_list.loc[64]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba66af30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # saving all forms locally \n",
    "# for i in range(max_row):\n",
    "#     text = requests.get(cik_list.SECFNAME[i]).text\n",
    "#     file_name = 'form' + str(i)\n",
    "#     f = open(file_name, 'a+')\n",
    "#     f.write(text)\n",
    "#     f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f8c22d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(max_row):\n",
    "    #print(i)\n",
    "    file_name = './form/form' + str(i)\n",
    "    text = open(file_name,'r').read()\n",
    "    print('reading..',end = \" \")\n",
    "    \n",
    "    #constraining_words_whole_report\n",
    "    \n",
    "#     constraining_words_whole_report_count = 0\n",
    "#     for word in denoise_text(text).split():\n",
    "#         if word in constraining_dict:\n",
    "#             constraining_words_whole_report_count += 1\n",
    "#     print('here...',end = \"  \")\n",
    "#     constraining_words_whole_report.loc[i] = constraining_words_whole_report_count\n",
    "    \n",
    "    ####################################\n",
    "    df.loc[i] = np.zeros(42)\n",
    "    # other variable per sections\n",
    "    for j in range(3):\n",
    "        if i in [63,64]:\n",
    "            continue\n",
    "        print(i,j,sep= '|',end = \" \")\n",
    "        exp = r\".*(?P<start>ITEM [\\d]\\. \" + re.escape(section[j]) + r\")(?P<MDA>.*)(?P<body>[\\s\\S]*)(?P<end>ITEM \\d|SIGNATURES)\"\n",
    "        regexp = re.compile(exp)\n",
    "        s = regexp.search(text)\n",
    "        \n",
    "        if s:\n",
    "            data = s.group('body')\n",
    "            text = denoise_text(data)\n",
    "            sent_list = sent_tokenize(text)\n",
    "            sentence_length = len(sent_list)\n",
    "\n",
    "            sample = text.split()\n",
    "            sample = normalize(sample)\n",
    "            word_count = len(sample)\n",
    "            complex_word_count = 0\n",
    "            \n",
    "            for word in sample:\n",
    "                if nsyl(word.lower()) > 2:\n",
    "                    complex_word_count += 1\n",
    "            \n",
    "            average_sentence_length = word_count/sentence_length\n",
    "            percentage_of_complex_words = complex_word_count/word_count\n",
    "            fog_index = 0.4 * (average_sentence_length + percentage_of_complex_words)\n",
    "            \n",
    "            positive_score = 0\n",
    "            negative_score = 0\n",
    "            uncertainty_score = 0\n",
    "            constraining_score = 0\n",
    "            for word in sample:\n",
    "                if word in master_dict.index:\n",
    "                    #print(\"is here\")\n",
    "                    if master_dict.loc[word].Positive > 0:\n",
    "                        #print(\"positive\")\n",
    "                        positive_score += 1\n",
    "                    if master_dict.loc[word].Negative > 0:\n",
    "                        negative_score += 1\n",
    "                    if word in uncertainty_dict:\n",
    "                        uncertainty_score += 1\n",
    "                    if word in constraining_dict:\n",
    "                        constraining_score += 1\n",
    "            #print(positive_score)\n",
    "            polarity_score = (positive_score-negative_score)/(positive_score + negative_score + .000001)\n",
    "            positive_word_proportion = positive_score/word_count\n",
    "            negative_word_proportion = negative_score/word_count\n",
    "            uncertainty_word_proportion = uncertainty_score/word_count\n",
    "            constraining_word_proportion = constraining_score/word_count\n",
    "            \n",
    "            df.loc[i][section_name[j].lower() + \"_positive_score\"] = positive_score\n",
    "            df.loc[i][section_name[j].lower() + \"_negative_score\"] = negative_score\n",
    "            df.loc[i][section_name[j].lower() + \"_polarity_score\"] = polarity_score\n",
    "            df.loc[i][section_name[j].lower() + \"_average_sentence_length\"] = average_sentence_length\n",
    "            df.loc[i][section_name[j].lower() + \"_percentage_of_complex_words\"] = percentage_of_complex_words\n",
    "            df.loc[i][section_name[j].lower() + \"_fog_index\"] = fog_index\n",
    "            df.loc[i][section_name[j].lower() + \"_complex_word_count\"] = complex_word_count\n",
    "            df.loc[i][section_name[j].lower() + \"_word_count\"] = word_count\n",
    "            df.loc[i][section_name[j].lower() + \"_uncertainty_score\"] = uncertainty_score\n",
    "            df.loc[i][section_name[j].lower() + \"_constraining_score\"] = constraining_score\n",
    "            df.loc[i][section_name[j].lower() + \"_positive_word_proportion\"] = positive_word_proportion\n",
    "            df.loc[i][section_name[j].lower() + \"_negative_word_proportion\"] = negative_word_proportion\n",
    "            df.loc[i][section_name[j].lower() + \"_uncertainty_word_proportion\"] = uncertainty_word_proportion\n",
    "            df.loc[i][section_name[j].lower() + \"_constraining_word_proportion\"] = constraining_word_proportion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f99284",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(max_row):\n",
    "    print(i,end = \" \")\n",
    "    file_name = './form/form' + str(i)\n",
    "    text = open(file_name,'r').read()\n",
    "    print('reading..',end = \" \")\n",
    "    \n",
    "    #constraining_words_whole_report\n",
    "    constraining_words_whole_report.loc[i] = 0\n",
    "    constraining_words_whole_report_count = 0\n",
    "    for word in denoise_text(text).split():\n",
    "        if word in constraining_dict:\n",
    "            constraining_words_whole_report_count += 1\n",
    "    print('here...',end = \"  \")\n",
    "    constraining_words_whole_report.loc[i] = constraining_words_whole_report_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d863a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# joing the files for output formate\n",
    "\n",
    "df = pd.concat([cik_list,df,constraining_words_whole_report], axis = 1)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a37b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f036736",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = pd.ExcelWriter('./output.xlsx')\n",
    "df.to_excel(writer, sheet_name='output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d54c150",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367fc676",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
